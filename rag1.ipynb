{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSCTytYXtnqIxUEG49BFGI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joseph-Moon/hello-world/blob/master/rag1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Jo6nS4szgDP",
        "outputId": "35bb385a-75ef-40de-829c-94be204c3201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.8/355.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.4/328.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip3 install -qU  markdownify  langchain-upstage rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NVelNgpzy8X",
        "outputId": "b082df59-4bf7-4950-b1f9-ee076baa95ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet openai python-dotenv"
      ],
      "metadata": {
        "id": "SjBsmDE00F_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv('/content/drive/MyDrive/Colab Notebooks/.env')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9gYl9Hs0BBg",
        "outputId": "3340044c-a048-4526-cedd-f64f08ef2b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_upstage import UpstageLayoutAnalysisLoader\n",
        "\n",
        "layzer = UpstageLayoutAnalysisLoader(\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/GenAI_Handbook.pdf\", use_ocr=True, output_type=\"html\"\n",
        ")\n",
        "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
        "docs = layzer.load()  # or layzer.lazy_load()"
      ],
      "metadata": {
        "id": "nbx94r5F0PjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "display(HTML(docs[0].page_content[:1000]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "OVygzEPK0brW",
        "outputId": "d3477e05-0359-405a-9b0b-18eea05ff2f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h1 id='0' style='font-size:22px'>Generative AI Handbook</h1><h1 id='1' style='font-size:20px'>1 Application Considerations</h1><h1 id='2' style='font-size:20px'>1. Prompt Management</h1><p id='3' data-category='paragraph' style='font-size:16px'>· Define a process for storing and testing several prompts. This is used to simplify<br>the prompt evaluation process.</p><br><p id='4' data-category='paragraph' style='font-size:18px'>2. Evaluation Framework</p><p id='5' data-category='paragraph' style='font-size:16px'>· Define key evaluation metrics<br>· Define a process for calculating these metrics using variable prompts</p><br><h1 id='6' style='font-size:18px'>3. Application Framework</h1><p id='7' data-category='paragraph' style='font-size:16px'>· Define if the application will leverage native APIs or external frameworks</p><br><p id='8' data-category='paragraph' style='font-size:18px'>4. Token Consumption</p><p id='9' data-category='paragraph' style='font-size:16px'>· Define a process fo"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_upstage import ChatUpstage\n",
        "\n",
        "llm = ChatUpstage()\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Please provide most correct answer from the following context.\n",
        "    If the answer is not present in the context, please write \"The information is not present in the context.\"\n",
        "    ---\n",
        "    Question: {question}\n",
        "    ---\n",
        "    Context: {Context}\n",
        "    \"\"\"\n",
        ")\n",
        "chain = prompt_template | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "IYCM7JaL0dan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U --quiet langchain_community langchain_text_splitters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWcnGb7707on",
        "outputId": "6921e3db-eb12-46d0-b49a-81a06f6da5a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.6/983.6 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain_text_splitters import (\n",
        "    Language,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    chunk_size=1000, chunk_overlap=100, language=Language.HTML\n",
        ")\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "retriever = BM25Retriever.from_documents(splits)"
      ],
      "metadata": {
        "id": "nHzgh1ep0qye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what do developers do to reduce the number of LLM requests?\"\n",
        "context_docs = retriever.invoke(query)\n",
        "chain.invoke({\"question\": query, \"Context\": context_docs})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "I9r-Dtdj1f0_",
        "outputId": "c2a47d82-580a-4937-8d07-5e3dad324e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Developers can reduce the number of LLM requests by batching requests or implementing delays between subsequent requests. Additionally, they should implement a process to track the number of requests sent or identify this from the error message/API endpoint to determine the degree to which they need to limit the requests.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is Parent-Child Document Retriever?\"\n",
        "context_docs = retriever.invoke(query)\n",
        "chain.invoke({\"question\": query, \"Context\": context_docs})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "pmam6RQx1neu",
        "outputId": "cf6faca9-52d5-4a56-ff45-a243f85ac1b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Parent-Child Document Retriever is a component of the Retriever in RAG (Retrieve and Generate) applications that utilizes a multi-level chunking strategy. It addresses the challenge of determining the optimal chunk size for vector similarity search and LLM prompt inclusion. The Parent-Child Document Retriever creates two chunk sizes and two chunk overlaps: one for large chunks and one for small chunks. The original data is first split into large chunks, and then the large chunks are split into small chunks. The small chunks contain a reference to the large chunks they were derived from. The small chunks are used to create vector embeddings, which are used during similarity search. This component of the retriever (child) enables the application to realize the benefits of smaller chunks, which contain less noise. After the application retrieves the most similar small chunks, it obtains the corresponding large chunks, and includes these chunks in the LLM prompt. This component of the retriever (parent) enables the application to realize the benefits of larger chunks, which provide adequate context for the LLM prompt.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Why do the RAG applications benefit from using a Parent-Child Document Retriever?\"\n",
        "context_docs = retriever.invoke(query)\n",
        "chain.invoke({\"question\": query, \"Context\": context_docs})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GFUSHdc31udo",
        "outputId": "d98e2736-c1b1-4665-fd82-bfae07d797ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The information is not present in the context.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}